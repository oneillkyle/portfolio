# ðŸ“š QA-on-Wikipedia: Retrieval-Augmented T5 Pipeline

An end-to-end repository for building a factual question-answering system over Wikipedia:

- **Domain-adaptive pre-training** on Wikipedia text (T5)  
- **Semantic cleaning** of Natural Questions data  
- **BM25 retrieval** via Elasticsearch  
- **Fine-tuning** a T5 model on cleaned QA pairs  
- **Inference** with context-augmented prompts  
- **Batch evaluation** (BLEU & ROUGE)  
- **Containerized deployment** (Docker & Docker Compose)  
- **CI/CD** (GitHub Actions & GitLab CI)

---

## ðŸ—‚ï¸ Repository Structure

```
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ wiki_json/                  â† WikiExtractor JSON output
â”‚   â””â”€â”€ cleaned_nq.jsonl            â† QA pairs after semantic cleaning
â”‚
â”œâ”€â”€ index_wiki_passages.py          â† Bulk-indexes wiki paragraphs into ES
â”œâ”€â”€ semantic_cleaner_advanced.py    â† Filters & dedups NQ examples
â”œâ”€â”€ split_jsonl.py                  â† Train/val split for JSONL
â”‚
â”œâ”€â”€ train_with_t5.py                â† Fine-tunes T5 on QA data
â”œâ”€â”€ evaluate_bleu_rouge.py          â† Computes BLEU & ROUGE scores
â”‚
â”œâ”€â”€ predict_t5.py                   â† Interactive CLI with beam search
â”œâ”€â”€ batch_predict_t5.py             â† Batch prediction (JSONL & CSV)
â”œâ”€â”€ fastapi_t5.py                   â† FastAPI endpoints: /predict & /batch_predict
â”‚
â”œâ”€â”€ Dockerfile                      â† Production container for API & indexer
â”œâ”€â”€ docker-compose.yml              â† Elasticsearch, indexer & API services
â”‚
â”œâ”€â”€ .github/workflows/              â† GitHub Actions CI/CD pipelines
â””â”€â”€ .gitlab-ci.yml                  â† GitLab CI example
```

---

## âš™ï¸ Setup & Dependencies

1. **Python 3.10+**  
2. **Elasticsearch 7.x**  
3. **CUDA & GPU** (optional, for faster training/inference)

Install core Python deps:

```bash
pip install   fastapi uvicorn transformers torch   sentence-transformers elasticsearch   datasets nltk rouge-score tqdm
```

---

## ðŸ“– Data Preparation

1. **Extract Wikipedia** (using WikiExtractor) into `data/wiki_json/`  
2. **Semantic cleaning** of NQ:
   ```bash
   python semantic_cleaner_advanced.py      --input simplified-nq-train.jsonl      --output data/cleaned_nq.jsonl      --limit 100000
   ```
3. **Split** into train/val:
   ```bash
   python split_jsonl.py      --input data/cleaned_nq.jsonl      --train_out data/cleaned_nq.train.jsonl      --val_out data/cleaned_nq.val.jsonl      --train_ratio 0.9
   ```

---

## ðŸ—„ï¸ Indexing with Elasticsearch

Build and populate a BM25 index of Wikipedia paragraphs:

```bash
docker-compose up -d elasticsearch
python index_wiki_passages.py   --wiki_dir data/wiki_json/   --es_host http://localhost:9200   --index qa_passages
```

---

## ðŸ¤– Model Training

### 1. **Pre-training (optional)**
Continue pre-training T5 on Wikipedia corpus:

Optionally take a subsample of the corpus for training.
```bash
shuf -n 1000000 ai/datasets/wiki_corpus.txt > ai/datasets/wiki_corpus.subsample.txt
```

```bash
python pretrain_t5_wiki.py
# outputs â†’ t5_wiki_pretrain/
```

### 2. **Fine-tuning on QA**

```bash
python train_with_t5.py
# loads from t5-small (or t5_wiki_pretrain/)
# outputs â†’ t5_trained_nq/
```

---

## ðŸ“Š Evaluation

Compute BLEU & ROUGE on validation set:

```bash
python evaluate_bleu_rouge.py   --model_dir t5_trained_nq   --data data/cleaned_nq.val.jsonl   --max_len 64 --beams 4
```

Use `results_visualizer.py` or React dashboard for plots.

---

## ðŸš€ Inference

### Interactive CLI

```bash
python predict_t5.py   --model_dir t5_trained_nq   --device cuda   --num_beams 5
```

### Batch

```bash
python batch_predict_t5.py   --model_dir t5_trained_nq   --input example_eval.jsonl   --output predictions.csv
```

### FastAPI Server

```bash
uvicorn fastapi_t5:app --host 0.0.0.0 --port 8000 --reload
```

- **POST** `/predict` â†’ `{ "question": "..." }`  
- **POST** `/batch_predict` â†’ `[{"question":"..."}, â€¦]`

---

## ðŸ“¦ Deployment

1. Place your `t5_trained_nq/` under `./models/`  
2. Ensure `./wiki_json/` and `docker-compose.yml` are present  
3. Launch all services:

   ```bash
   docker-compose up --build
   ```

Access the API at **http://localhost:8000/docs**.

---

## ðŸ”„ CI / CD

- **GitHub Actions**: see `.github/workflows/ci.yml` & `cd.yml`  
- **GitLab CI**: see `.gitlab-ci.yml`

On every push to **main**, code is linted, tested, Docker images builtâ€”and on deploy, containers are updated via SSH.

---

## ðŸ“š Further Reading

- â€œRetrieval-Augmented Generationâ€ (RAG): https://arxiv.org/abs/2005.11401  
- Hugging Face T5 tutorial: https://huggingface.co/docs/transformers/model_doc/t5  
- Elasticsearch BM25 & vector search docs

---

> **â€œA model is only as good as its data and grounding.â€**  
> By combining Wikipedia pre-training with explicit retrieval, youâ€™ll get both broad knowledge and up-to-date facts. Enjoy building!
